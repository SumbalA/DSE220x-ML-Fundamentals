© 2012–2018 edX Inc. All rights reserved except where noted. EdX, Open edX and the edX and Open edX logos are registered trademarks or trademarks of edX Inc. | 粤ICP备17044299号-2

Powered by Open edX

Welcome to Machine Learning Fundamentals!
We are delighted to welcome you to the third course in the MicroMasters in Data Science: Machine Learning Fundamentals. In this course, you will learn the motivation, intuition, and theory behind the probabilistic and statistical foundations of data science, and will get to experiment and practice with these concepts via Python programs and the Jupyter Notebook platform. 

Course Staff
Instructor
Sanjoy Dasgupta, Professor, Computer Science and Engineering Department, UC San Diego

What do you need to know to succeed?
The most important pre-requisites for this course are:

The ability to program in Python and to use Jupyter notebooks. This can be obtained by taking the course DSE200x, Python for Data Science.

Familiarity with calculus, especially derivatives of single-variable and multivariate functions.

The course will make heavy use of basic probability and linear algebra. Although we will introduce these concepts as needed, it will be easiest if learners already have some familiarity with them.

Learning objectives

This course is an intensive introduction to the most widely-used machine learning methods. The first goal is to provide a basic intuitive understanding of these techniques: what they are good for, how they work, how they relate to one another, and their strengths and weaknesses. The second goal is to provide a hands-on feel for these methods through experiments with suitable data sets, using Jupyter notebooks. The third goal is to understand machine learning methods at a deeper level by delving into their mathematical underpinnings. This is crucial to being able to adapt and modify existing methods and to creatively combining them.

__________________________________________________________________________________________________________

Overview

Topics:

Taxonomy of prediction problems
Nearest neighbor methods and families of distance functions
Generalization: what it means; overfitting; selecting parameters using cross-validation
Generative modeling for classification, especially using the multivariate Gaussian
Linear regression and its variants
Logistic regression
Optimization: deriving stochastic gradient descent algorithms and testing convexity
Linear classification using the support vector machine
Nonlinear modeling using basis expansion and kernel methods
Decision trees, boosting, and random forests
Methods for flat and hierarchical clustering
Principal component analysis 
Autoencoders, distributed representations, and deep learning

Course Outline

This is a ten-week course.

Week 1: Introduction: nearest neighbor, and a host of prediction problems
Week 2: Probability basics and generative modeling
Week 3: Linear algebra basics, the multivariate Gaussian, and more generative modeling
Week 4: Linear regression and logistic regression
Week 5: Optimization
Week 6: Support vector machines
Week 7: Beyond linear prediction: kernel methods, decision trees, boosting, random forests
Week 8: Clustering
Week 9: Informative projections
Week 10: Deep learning
